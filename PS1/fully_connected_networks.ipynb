{"cells":[{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"DDJwQPZcupab","new_sheet":false,"run_control":{"read_only":false}},"source":["# CS 639 Problem set 1-3: Fully-Connected Neural Networks (180 points)\n","\n","Before we start, please put your full name(s), your UW NetID and your 10-digit student ID.\n","\n","1. Firstname1 LASTNAME1, NetID1,    //   e.g.) Josef PIEPER,jpieper276, 0123456789\n","2. Firstname2 LASTNAME2, NetID2,    //   e.g.) Mel GIBSON, ,gibson65, 9876543210\n","\n","If you do not have a teammate, leave the second entry empty"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"tt7vq1h6mRto","new_sheet":false,"run_control":{"read_only":false}},"source":["**Your Answer:**   \n","1.\n","2."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"FrfeHl_-m4V-","new_sheet":false,"run_control":{"read_only":false}},"source":["## Setup Code\n","Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"VyQblYp0nEZq","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"Q7ymI0aZ2W1b","new_sheet":false,"run_control":{"read_only":false}},"source":["### Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"c_LLpLyC2eau","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"mbq-UT8J2mnv","new_sheet":false,"run_control":{"read_only":false}},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"WcrhTOZW243H","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a CS_639 folder and put all the files under PS1 folder, then 'CS639/PS1'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'CS639/PS1'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"xegb0uDA232J","new_sheet":false,"run_control":{"read_only":false}},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Hello from fully_connected_networks.py!\n","Hello from ps1_helper.py!\n","```\n","\n","as well as the last edit time for the file `fully_connected_networks.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"AhGQF5sw3Fas","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Central\"\n","time.tzset()\n","\n","from fully_connected_networks import hello_fully_connected_networks\n","hello_fully_connected_networks()\n","\n","from ps1_helper import hello_helper\n","hello_helper()\n","\n","fully_connected_networks_path = os.path.join(GOOGLE_DRIVE_PATH, 'fully_connected_networks.py')\n","fully_connected_networks_edit_time = time.ctime(os.path.getmtime(fully_connected_networks_path))\n","print('fully_connected_networks.py last edited on %s' % fully_connected_networks_edit_time)\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ynKS05gJ4iBo","new_sheet":false,"run_control":{"read_only":false}},"source":["# Data preprocessing"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"fN1SShPR4lJV","new_sheet":false,"run_control":{"read_only":false}},"source":["## Setup code\n","Run some setup code for this notebook: Import some useful packages and increase the default figure size."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"VUCKw4Tl1ddj","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import cs639\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import statistics\n","import random\n","import time\n","import math\n","%matplotlib inline\n","from cs639 import reset_seed, Solver\n","\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","plt.rcParams['font.size'] = 16"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"lhqpd2IN2O-K","new_sheet":false,"run_control":{"read_only":false}},"source":["Starting in this assignment, we will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"SGDxdBIMRX6b","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["if torch.cuda.is_available:\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"-Yv3zQYw5B3s","new_sheet":false,"run_control":{"read_only":false}},"source":["## Load the CIFAR-10 dataset\n","Then, we will first load the CIFAR-10 dataset. The utility function `cs639.data.preprocess_cifar10()` returns the entire CIFAR-10 dataset as a set of six **Torch tensors** while also preprocessing the RGB images:\n","\n","- `X_train` contains all training images (real numbers in the range $[0, 1]$)\n","- `y_train` contains all training labels (integers in the range $[0, 9]$)\n","- `X_val` contains all validation images\n","- `y_val` contains all validation labels\n","- `X_test` contains all test images\n","- `y_test` contains all test labels"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"V2mFlFmQ1ddm","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Invoke the above function to get our data.\n","import cs639\n","\n","cs639.reset_seed(0)\n","data_dict = cs639.data.preprocess_cifar10(cuda=True, dtype=torch.float64)\n","print('Train data shape: ', data_dict['X_train'].shape)\n","print('Train labels shape: ', data_dict['y_train'].shape)\n","print('Validation data shape: ', data_dict['X_val'].shape)\n","print('Validation labels shape: ', data_dict['y_val'].shape)\n","print('Test data shape: ', data_dict['X_test'].shape)\n","print('Test labels shape: ', data_dict['y_test'].shape)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ZeH0OvuEe1CN","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-title"]},"source":["# Fully-connected neural networks\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3Qiu9_4pe1CP","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-ignore"]},"source":["In this exercise we will implement fully-connected networks using a modular approach. The goal of such an approach will be to build networks using a design so that we can implement different layer types in isolation and then snap them together into models with different architectures\n","For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n","\n","```python\n","def forward(x, w):\n","  \"\"\" Receive inputs x and weights w \"\"\"\n","  # Do some computations ...\n","  z = # ... some intermediate value\n","  # Do some more computations ...\n","  out = # the output\n","   \n","  cache = (x, w, z, out) # Values we need to compute gradients\n","   \n","  return out, cache\n","```\n","\n","The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n","\n","```python\n","def backward(dout, cache):\n","  \"\"\"\n","  Receive dout (derivative of loss with respect to outputs) and cache,\n","  and compute derivative with respect to inputs.\n","  \"\"\"\n","  # Unpack cache values\n","  x, w, z, out = cache\n","  \n","  # Use values in cache to compute derivatives\n","  dx = # Derivative of loss with respect to x\n","  dw = # Derivative of loss with respect to w\n","  \n","  return dx, dw\n","```\n","\n","After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n","\n","\n","To validate our implementation, we will compare the analytically computed gradients with numerical approximations of the gradient as done in previous assignments. You can inspect the numeric gradient function `compute_numeric_gradient` in `cs639/grad.py`. Please note that we have updated the function to accept upstream gradients to allow us to debug intermediate layers easily. You can check the update description by running the code block below.\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"DfseUp7H-TF_","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["help(cs639.grad.compute_numeric_gradient)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"JB7Eu3qJ9xnm","new_sheet":false,"run_control":{"read_only":false}},"source":["# Linear layer"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"bRdnxsvZunFu","new_sheet":false,"run_control":{"read_only":false}},"source":["For each layer we implement, we will define a class with two static methods `forward` and `backward`. The class structure is currently provided in `fully_connected_layers.py`, you will be implementing both the `forward` and `backward` methods."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"0NNv3l-ne1Cb","new_sheet":false,"run_control":{"read_only":false}},"source":["## Linear layer: forward (20 points)\n","Implement the `Linear.forward` function in `fully_connected_layers.py`. Once you are done you can test your implementaion by running the next cell. You should see errors less than `1e-7`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"sjq2Sq4Ze1Cc","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Linear\n","\n","# Test the Linear.forward function\n","num_inputs = 3\n","input_shape = torch.tensor((4, 4))\n","output_dim = 5\n","\n","input_size = num_inputs * torch.prod(input_shape)\n","weight_size = output_dim * torch.prod(input_shape)\n","\n","x = torch.linspace(-0.3, 0.3, steps=input_size, dtype=torch.float64, device='cuda')\n","w = torch.linspace(-0.2, 0.4, steps=weight_size, dtype=torch.float64, device='cuda')\n","b = torch.linspace(-0.1, 0.5, steps=output_dim, dtype=torch.float64, device='cuda')\n","x = x.reshape(num_inputs, *input_shape)\n","w = w.reshape(torch.prod(input_shape), output_dim)\n","\n","out, _ = Linear.forward(x, w, b)\n","correct_out = torch.tensor([[-0.21234043, -0.08716133,  0.03801778,  0.16319688,  0.28837598],\n","                            [ 0.06482629,  0.21482629,  0.36482629,  0.51482629,  0.66482629],\n","                            [ 0.34199300,  0.51681390,  0.69163480,  0.86645570,  1.04127660]]\n","                            ).double().cuda()\n","\n","print('Testing Linear.forward function:')\n","print('difference: ', cs639.grad.rel_error(out, correct_out))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"4mxIDo46e1Cf","new_sheet":false,"run_control":{"read_only":false}},"source":["## Linear layer: backward (30 points)\n","Now implement the `Linear.backward` function and test your implementation using numeric gradient checking.\n","\n","Run the following to test your implementation of `Linear.backward`. You should see errors less than `1e-7`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"ts85gmote1Cg","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Linear\n","\n","# Test the Linear.backward function\n","reset_seed(0)\n","x = torch.randn(5, 3, 4, dtype=torch.float64, device='cuda')\n","w = torch.randn(12, 7, dtype=torch.float64, device='cuda')\n","b = torch.randn(7, dtype=torch.float64, device='cuda')\n","dout = torch.randn(5, 7, dtype=torch.float64, device='cuda')\n","\n","dx_num = cs639.grad.compute_numeric_gradient(lambda x: Linear.forward(x, w, b)[0], x, dout)\n","dw_num = cs639.grad.compute_numeric_gradient(lambda w: Linear.forward(x, w, b)[0], w, dout)\n","db_num = cs639.grad.compute_numeric_gradient(lambda b: Linear.forward(x, w, b)[0], b, dout)\n","\n","_, cache = Linear.forward(x, w, b)\n","dx, dw, db = Linear.backward(dout, cache)\n","\n","# The error should be around e-10 or less\n","print('Testing Linear.backward function:')\n","print('dx error: ', cs639.grad.rel_error(dx_num, dx))\n","print('dw error: ', cs639.grad.rel_error(dw_num, dw))\n","print('db error: ', cs639.grad.rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"bdIqQzqiJQE6","new_sheet":false,"run_control":{"read_only":false}},"source":["# ReLU activation"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"YdX98A_qvTRt","new_sheet":false,"run_control":{"read_only":false}},"source":["We will now implement the ReLU nonlinearity. As above, we will define a class with two empty static methods, and implement them in upcoming cells. The class structure can be found in `fully_connected_networks.py`"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"n2DyqL4Ae1Cl","new_sheet":false,"run_control":{"read_only":false}},"source":["## ReLU activation: forward (10 points)\n","Implement the forward pass for the ReLU activation function in the `ReLU.forward` function. You **should not** change the input tensor with an in-place operation.\n","\n","Run the following to test your implementation of the ReLU forward pass. Your errors should be less than `1e-7`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"QblpieUJe1Cm","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import ReLU\n","\n","reset_seed(0)\n","x = torch.linspace(-1.0, 1.0, steps=15, dtype=torch.float64, device='cuda')\n","x = x.reshape(5, 3)\n","\n","out, _ = ReLU.forward(x)\n","correct_out = torch.tensor([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","                            [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n","                            [0.00000000e+00, 5.55111512e-17, 1.42857143e-01],\n","                            [2.85714286e-01, 4.28571429e-01, 5.71428571e-01],\n","                            [7.14285714e-01, 8.57142857e-01, 1.00000000e+00]\n","                            ],\n","                            dtype=torch.float64,\n","                            device='cuda')\n","\n","# Compare your output with ours. The error should be on the order of e-8\n","print('Testing ReLU.forward function:')\n","print('difference: ', cs639.grad.rel_error(out, correct_out))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3bSInb7xe1Cq","new_sheet":false,"run_control":{"read_only":false}},"source":["## ReLU activation: backward (10 points)\n","Now implement the backward pass for the ReLU activation function.\n","\n","Again, you should not change the input tensor with an in-place operation.\n","\n","Run the following to test your implementation of `ReLU.backward`. Your errors should be less than `1e-8`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"odiV48zBe1Cr","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import ReLU\n","\n","reset_seed(0)\n","x = torch.randn(5, 5, dtype=torch.float64, device='cuda')\n","dout = torch.randn(*x.shape, dtype=torch.float64, device='cuda')\n","\n","dx_num = cs639.grad.compute_numeric_gradient(lambda x: ReLU.forward(x)[0], x, dout)\n","\n","_, cache = ReLU.forward(x)\n","dx = ReLU.backward(dout, cache)\n","\n","# The error should be on the order of e-12\n","print('Testing ReLU.backward function:')\n","print('dx error: ', cs639.grad.rel_error(dx_num, dx))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"eVTMuUOZe1Cv","new_sheet":false,"run_control":{"read_only":false}},"source":["# \"Sandwich\" layers\n","There are some common patterns of layers that are frequently used in neural nets. For example, linear layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define a convenience layer.\n","\n","This also shows how our layer abstraction allows us to implement new layers by composing existing layer implementations. This is a powerful mechanism for structuring deep learning code in a modular fashion.\n","\n","For now take a look at the `forward` and `backward` functions in `Linear_ReLU`, and run the following to numerically gradient check the backward pass.\n","\n","Run the following to test the implementation of the `Linear_ReLU` layer using numeric gradient checking. You should see errors less than `1e-8`"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"-gaY5YfAe1Cw","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Linear_ReLU\n","\n","reset_seed(0)\n","x = torch.randn(5, 2, 3, dtype=torch.float64, device='cuda')\n","w = torch.randn(6, 8, dtype=torch.float64, device='cuda')\n","b = torch.randn(8, dtype=torch.float64, device='cuda')\n","dout = torch.randn(5, 8, dtype=torch.float64, device='cuda')\n","\n","out, cache = Linear_ReLU.forward(x, w, b)\n","dx, dw, db = Linear_ReLU.backward(dout, cache)\n","\n","dx_num = cs639.grad.compute_numeric_gradient(lambda x: Linear_ReLU.forward(x, w, b)[0], x, dout)\n","dw_num = cs639.grad.compute_numeric_gradient(lambda w: Linear_ReLU.forward(x, w, b)[0], w, dout)\n","db_num = cs639.grad.compute_numeric_gradient(lambda b: Linear_ReLU.forward(x, w, b)[0], b, dout)\n","\n","# Relative error should be around e-8 or less\n","print('Testing Linear_ReLU.forward and Linear_ReLU.backward:')\n","print('dx error: ', cs639.grad.rel_error(dx_num, dx))\n","print('dw error: ', cs639.grad.rel_error(dw_num, dw))\n","print('db error: ', cs639.grad.rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"rAGgiyP5e1C0","new_sheet":false,"run_control":{"read_only":false}},"source":["# Loss layers: Softmax\n","You implemented the softmax loss functions in the last assignment, so we'll give it to you for free in `ps1_helper.py`. You should still make sure you understand how it works by looking at the implementations. We can first verify our implementation.\n","\n","\n","Run the following to perform numeric gradient checking on the two loss functions. You should see errors less than `1e-6` for softmax_loss."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"BU9xp64De1C1","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from ps1_helper import softmax_loss\n","\n","reset_seed(0)\n","num_classes, num_inputs = 10, 10\n","x = 0.0005 * torch.randn(num_inputs, num_classes, dtype=torch.float64, device='cuda')\n","y = torch.randint(num_classes, size=(num_inputs,), dtype=torch.int64, device='cuda')\n","\n","dx_num = cs639.grad.compute_numeric_gradient(lambda x: softmax_loss(x, y)[0], x)\n","loss, dx = softmax_loss(x, y)\n","\n","# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n","print('\\nTesting softmax_loss:')\n","print('loss: ', loss.item())\n","print('dx error: ', cs639.grad.rel_error(dx_num, dx))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"qq7-cyfQe1C4","new_sheet":false,"run_control":{"read_only":false}},"source":["# Two-layer network (50 points)\n","You will implement the two layer network using modular implementations.\n","\n","Complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API.\n","\n","Once you have finished implementing the forward and backward passes of your two-layer net, run the following to test your implementation:"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"d3JOcfyze1C5","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import TwoLayerNet\n","from ps1_helper import softmax_loss\n","\n","reset_seed(0)\n","N, D, H, C = 5, 10, 20, 4\n","X = torch.randn(N, D, dtype=torch.float64, device='cuda')\n","y = torch.randint(C, size=(N,), dtype=torch.int64, device='cuda')\n","\n","std = 1e-3\n","model = TwoLayerNet(\n","          input_dim=D,\n","          hidden_dim=H,\n","          num_classes=C,\n","          weight_scale=std,\n","          dtype=torch.float64,\n","          device='cuda'\n","        )\n","\n","print('Testing initialization ... ')\n","W1_std = torch.abs(model.params['W1'].std() - std)\n","b1 = model.params['b1']\n","W2_std = torch.abs(model.params['W2'].std() - std)\n","b2 = model.params['b2']\n","assert W1_std < std / 10, 'First layer weights do not seem right'\n","assert torch.all(b1 == 0), 'First layer biases do not seem right'\n","assert W2_std < std / 10, 'Second layer weights do not seem right'\n","assert torch.all(b2 == 0), 'Second layer biases do not seem right'\n","\n","print('Testing test-time forward pass ... ')\n","model.params['W1'] = torch.linspace(-0.2, 0.2, steps=D * H, dtype=torch.float64, device='cuda').reshape(D, H)\n","model.params['b1'] = torch.linspace(-0.3, 0.3, steps=H, dtype=torch.float64, device='cuda')\n","model.params['W2'] = torch.linspace(-0.4, 0.4, steps=H * C, dtype=torch.float64, device='cuda').reshape(H, C)\n","model.params['b2'] = torch.linspace(-0.7, 0.7, steps=C, dtype=torch.float64, device='cuda')\n","X = torch.linspace(-0.5, 7.7, steps=N * D, dtype=torch.float64, device='cuda').reshape(D, N).t()\n","scores = model.loss(X)\n","\n","correct_scores = torch.tensor(\n","  [[1.07553643, 2.10424866, 3.13296090, 4.16167313],\n","   [1.16614498, 2.19485722, 3.22356945, 4.25228169],\n","   [1.25675354, 2.28546578, 3.31417801, 4.34289025],\n","   [1.34736210, 2.37607434, 3.40478657, 4.43349880],\n","   [1.43797066, 2.46668289, 3.49539513, 4.52410736]],\n","    dtype=torch.float64, device='cuda')\n","scores_diff = torch.abs(scores - correct_scores).sum()\n","assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n","\n","print('Testing training loss (no regularization)')\n","y = torch.tensor([0, 0, 1, 2, 3])\n","loss, grads = model.loss(X, y)\n","torch.set_printoptions(precision=10)\n","\n","correct_loss = 2.2775559358\n","assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n","\n","model.reg = 4.0\n","loss, grads = model.loss(X, y)\n","correct_loss = 30.5501594598\n","assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n","\n","# Errors should be around e-6 or less\n","for reg in [0.0, 1.0]:\n","  print('Running numeric gradient check with reg = ', reg)\n","  model.reg = reg\n","  loss, grads = model.loss(X, y)\n","\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = cs639.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, cs639.grad.rel_error(grad_num, grads[name])))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"q1Odj9XQe1C9","new_sheet":false,"run_control":{"read_only":false}},"source":["# Solver (20 points)\n","In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n","\n","Read through `help(Solver)` to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"lZ-8wKffRoDu","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["print(help(Solver))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ldQOcCK46YYx","new_sheet":false,"run_control":{"read_only":false}},"source":["Use the Solver classe to create a solver instance that trains a TwoLayerNet to achieve at least 50% performance on the validation set.\n","\n","**Implement** `create_solver_instance` in `fully_connected_networks.py` to return a solver instance. Make sure to initialize the Solver instance with the input device."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"6unJrOule1C_","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import create_solver_instance\n","\n","reset_seed(0)\n","\n","# Create a solver instance that achieves 50% performance on the validation set\n","solver = create_solver_instance(data_dict=data_dict, dtype=torch.float64, device='cuda')\n","solver.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"gSSy7LTde1DE","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Run this cell to visualize training loss and train / val accuracy\n","plt.subplot(2, 1, 1)\n","plt.title('Training loss')\n","plt.plot(solver.loss_history, 'o')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(2, 1, 2)\n","plt.title('Accuracy')\n","plt.plot(solver.train_acc_history, '-o', label='train')\n","plt.plot(solver.val_acc_history, '-o', label='val')\n","plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n","plt.xlabel('Epoch')\n","plt.legend(loc='lower right')\n","plt.gcf().set_size_inches(15, 12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"oUwvMomE31Mh","new_sheet":false,"run_control":{"read_only":false}},"source":["If you're happy with the model's perfromance, run the following cell to save it.\n","\n","We will also reload the model and run it on validation to verify it's the right weights."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"AfE_2VVK31fa","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["path = os.path.join(GOOGLE_DRIVE_PATH, 'best_two_layer_net.pth')\n","solver.model.save(path)\n","\n","# Create a new instance\n","from fully_connected_networks import create_solver_instance\n","reset_seed(0)\n","\n","solver = create_solver_instance(data_dict=data_dict, dtype=torch.float64, device='cuda')\n","\n","# Load model\n","solver.model.load(path, dtype=torch.float64, device='cuda')\n","\n","# Evaluate on validation set\n","accuracy = solver.check_accuracy(solver.X_val, solver.y_val)\n","print(f\"Saved model's accuracy on validation is {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"eNyFLT1We1DI","new_sheet":false,"run_control":{"read_only":false}},"source":["# Multilayer network (40 points)\n","Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n","\n","Read through the `FullyConnectedNet` class in `fully_connected_networks.py`. Implement the initialization, the forward pass, and the backward pass."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3abR1_qhe1DK","new_sheet":false,"run_control":{"read_only":false}},"source":["## Initial loss and gradient check\n","\n","As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n","\n","For gradient checking, you should expect to see errors less than `1e-6`, except for the check on `W1` and `W2` with `reg=0` where your errors should be less than `1e-5`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"1waPtKRDe1DL","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet\n","\n","reset_seed(0)\n","N, D, H1, H2, C = 3, 10, 25, 15, 10\n","X = torch.randn(N, D, dtype=torch.float64, device='cuda')\n","y = torch.randint(C, size=(N,), dtype=torch.int64, device='cuda')\n","\n","for reg in [0, 2.78]:\n","  print('Running check with reg = ', reg)\n","  model = FullyConnectedNet(\n","        [H1, H2],\n","        input_dim=D,\n","        num_classes=C,\n","        reg=reg,\n","        weight_scale=5e-2,\n","        dtype=torch.float64,\n","        device='cuda'\n","  )\n","\n","  loss, grads = model.loss(X, y)\n","  print('Initial loss: ', loss.item())\n","\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = cs639.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, cs639.grad.rel_error(grad_num, grads[name])))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"-q6aWzNfe1DQ","new_sheet":false,"run_control":{"read_only":false}},"source":["As another sanity check, make sure you can overfit a small dataset of 100 images. First we will try a three-layer network with 50 units in each hidden layer. In the following cell, tweak the **learning rate** and **weight initialization scale** to overfit and achieve 100% training accuracy within 25 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"2NccCDJ3e1DR","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet, get_three_layer_network_params\n","\n","# TODO: Use a three-layer Net to overfit 100 training examples by\n","# tweaking just the learning rate and initialization scale.\n","reset_seed(0)\n","\n","num_train = 100\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","# Update parameters in get_three_layer_network_params\n","weight_scale, learning_rate = get_three_layer_network_params()\n","\n","model = FullyConnectedNet([50, 50],\n","              weight_scale=weight_scale, dtype=torch.float32, device='cuda')\n","solver = Solver(model, small_data,\n","                print_every=10, num_epochs=25, batch_size=25,\n","                optim_config={\n","                  'learning_rate': learning_rate,\n","                },\n","                device='cuda',\n","         )\n","solver.train()\n","\n","plt.plot(solver.loss_history, 'o')\n","plt.title('Training loss history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"tskjw8VUe1DV","new_sheet":false,"run_control":{"read_only":false}},"source":["Now try to use a five-layer network with 50 units on each layer to overfit 100 training examples. Again, you will have to adjust the learning rate and weight initialization scale, but you should be able to achieve 100% training accuracy within 40 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"D5mAWrrPe1Dc","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet, get_five_layer_network_params\n","\n","# TODO: Use a five-layer Net to overfit 100 training examples by\n","# tweaking just the learning rate and initialization scale.\n","reset_seed(0)\n","\n","num_train = 100\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","\n","# Update parameters in get_three_layer_network_params\n","weight_scale, learning_rate = get_five_layer_network_params()\n","\n","# Run models and solver with parameters\n","model = FullyConnectedNet([50, 50, 50, 50],\n","                weight_scale=weight_scale, dtype=torch.float32, device='cuda')\n","solver = Solver(model, small_data,\n","                print_every=10, num_epochs=40, batch_size=25,\n","                optim_config={\n","                  'learning_rate': learning_rate,\n","                },\n","                device='cuda',\n","         )\n","# Turn off keep_best_params to allow final weights to be saved, instead of best weights on validation set.\n","solver.train(return_best_params=False)\n","\n","plt.plot(solver.loss_history, 'o')\n","plt.title('Training loss history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3M2JQjj_93RW","new_sheet":false,"run_control":{"read_only":false}},"source":["If you're satisfied with your model's performance, save the overfit model. Just a sanity check, we evaluate it one the training set again to verify that the saved weights have the correct performance."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"YAnM8V9z938Q","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Set path\n","path = os.path.join(GOOGLE_DRIVE_PATH, 'best_overfit_five_layer_net.pth')\n","solver.model.save(path)\n","\n","\n","# Create a new instance  -- Note that hidden dims being different doesn't matter here.\n","model = FullyConnectedNet(hidden_dims=[100, ], dtype=torch.float32, device='cuda')\n","solver = Solver(model, small_data,\n","                print_every=10, num_epochs=20, batch_size=25,\n","                optim_config={\n","                  'learning_rate': learning_rate,\n","                },\n","                device='cuda',\n","         )\n","\n","\n","# Load model\n","solver.model.load(path, dtype=torch.float32, device='cuda')\n","\n","# Evaluate on validation set\n","accuracy = solver.check_accuracy(solver.X_train, solver.y_train)\n","print(f\"Saved model's accuracy on small train is {accuracy}\")\n"]},{"cell_type":"markdown","source":["## Submit Your Work\n","After completing this notebook, run the following cell to create a `.zip` file for you to download and turn in. **Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"],"metadata":{"id":"k1RwA3d3KjHA"}},{"cell_type":"code","source":["from cs639.submit import make_ps1_submission\n","\n","make_ps1_submission(GOOGLE_DRIVE_PATH)"],"metadata":{"id":"JMQZFsyvKun2"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"}},"nbformat":4,"nbformat_minor":0}