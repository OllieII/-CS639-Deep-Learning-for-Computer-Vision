{"cells":[{"cell_type":"markdown","metadata":{"id":"vTOX0d3cmLQf"},"source":["# CS 639 Problem set 1-2: Linear Classifiers (100 points)\n","\n","Before we start, please put your full name(s), your UW NetID and your 10-digit student ID.\n","\n","1. Firstname1 LASTNAME1, NetID1,    //   e.g.) Josef PIEPER,jpieper276, 0123456789\n","2. Firstname2 LASTNAME2, NetID2,    //   e.g.) Mel GIBSON, ,gibson65, 9876543210\n","\n","If you do not have a teammate, leave the second entry empty"]},{"cell_type":"markdown","metadata":{"id":"tt7vq1h6mRto"},"source":["**Your Answer:**   \n","1.\n","2."]},{"cell_type":"markdown","metadata":{"id":"FrfeHl_-m4V-"},"source":["## Setup Code\n","Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VyQblYp0nEZq"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"Q7ymI0aZ2W1b"},"source":["### Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_LLpLyC2eau"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"mbq-UT8J2mnv"},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n","\n","```\n","['linear_classifier.ipynb', 'cs639', 'knn.py', 'linear_classifier.py', 'ps1_helper.py', 'fully_connected_networks.ipynb', 'fully_connected_networks.py', 'knn.ipynb']\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WcrhTOZW243H"},"outputs":[],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a CS_639 folder and put all the files under PS1 folder, then 'CS_639/PS1'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'CS_639/PS1'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'CS_639/PS1'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"markdown","metadata":{"id":"xegb0uDA232J"},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run th following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Hello from linear_classifier.py!\n","```\n","\n","as well as the last edit time for the file `linear_classifier.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhGQF5sw3Fas"},"outputs":[],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Central\"\n","time.tzset()\n","\n","from linear_classifier import hello_linear_classifier\n","hello_linear_classifier()\n","\n","linear_classifier_path = os.path.join(GOOGLE_DRIVE_PATH, 'linear_classifier.py')\n","linear_classifier_edit_time = time.ctime(os.path.getmtime(linear_classifier_path))\n","print('linear_classifier.py last edited on %s' % linear_classifier_edit_time)"]},{"cell_type":"markdown","metadata":{"id":"ynKS05gJ4iBo"},"source":["# Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"fN1SShPR4lJV"},"source":["## Setup code\n","Run some setup code for this notebook: Import some useful packages and increase the default figure size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUCKw4Tl1ddj"},"outputs":[],"source":["import cs639\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import statistics\n","import random\n","import time\n","import math\n","%matplotlib inline\n","\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","plt.rcParams['font.size'] = 16"]},{"cell_type":"markdown","metadata":{"id":"lhqpd2IN2O-K"},"source":["Starting in this assignment, we will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGDxdBIMRX6b"},"outputs":[],"source":["if torch.cuda.is_available:\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"]},{"cell_type":"markdown","metadata":{"id":"-Yv3zQYw5B3s"},"source":["## Load the CIFAR-10 dataset\n","Then, we will first load the CIFAR-10 dataset, same as knn. The utility function `cs639.data.preprocess_cifar10()` returns the entire CIFAR-10 dataset as a set of six **Torch tensors**:\n","\n","- `X_train` contains all training images (real numbers in the range $[0, 1]$)\n","- `y_train` contains all training labels (integers in the range $[0, 9]$)\n","- `X_val` contains all validation images\n","- `y_val` contains all validation labels\n","- `X_test` contains all test images\n","- `y_test` contains all test labels\n","\n","In this notebook we will use the **bias trick**: By adding an extra constant feature of ones to each image, we avoid the need to keep track of a bias vector; the bias will be encoded as the part of the weight matrix that interacts with the constant ones in the input.\n","\n","In the `two_layer_net.ipynb` notebook that follows this one, we will not use the bias trick.\n","\n","We can learn more about the `cs639.data.preprocess_cifar10` function by invoking the `help` command:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2mFlFmQ1ddm"},"outputs":[],"source":["import cs639\n","help(cs639.data.preprocess_cifar10)"]},{"cell_type":"markdown","metadata":{"id":"COCx2kM6XB3K"},"source":["We can now run the `cs639.data.preprocess` function to get our data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_BhZ_6_XB3K"},"outputs":[],"source":["# Invoke the above function to get our data.\n","import cs639\n","\n","cs639.reset_seed(0)\n","data_dict = cs639.data.preprocess_cifar10(bias_trick=True, cuda=True, dtype=torch.float64)\n","print('Train data shape: ', data_dict['X_train'].shape)\n","print('Train labels shape: ', data_dict['y_train'].shape)\n","print('Validation data shape: ', data_dict['X_val'].shape)\n","print('Validation labels shape: ', data_dict['y_val'].shape)\n","print('Test data shape: ', data_dict['X_test'].shape)\n","print('Test labels shape: ', data_dict['y_test'].shape)"]},{"cell_type":"markdown","metadata":{"id":"DkuwyMY27RxS"},"source":["# Softmax Classifier\n","\n","In this section, you will:\n","\n","- implement a fully-vectorized **loss function** for the Softmax classifier\n","- implement the fully-vectorized expression for its **analytic gradient**\n","- **check your implementation** with numerical gradient\n","- use a validation set to **tune the learning rate and regularization** strength\n","- **optimize** the loss function with **SGD**\n","- **visualize** the final learned weights\n","\n","You SHOULD NOT use \".to()\" or \".cuda()\" in each implementation block."]},{"cell_type":"markdown","metadata":{"id":"hLJMVGtvIgo3"},"source":["First, let's start from implementing the naive softmax loss function with nested loops in `softmax_loss_naive` function. **(20 points)**"]},{"cell_type":"markdown","metadata":{"id":"cER8fiSq7Ys-"},"source":["As a sanity check to see whether we have implemented the loss correctly, run the softmax classifier with a small random weight matrix and no regularization. You should see loss near log(10) = 2.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9q77O7F7VI6"},"outputs":[],"source":["import cs639\n","from linear_classifier import softmax_loss_naive\n","\n","cs639.reset_seed(0)\n","# Generate a random softmax weight tensor and use it to compute the loss.\n","W = 0.0005 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n","\n","X_batch = data_dict['X_val'][:128]\n","y_batch = data_dict['y_val'][:128]\n","\n","# YOUR_TURN: Complete the implementation of softmax_loss_naive and implement\n","# a (naive) version of the gradient that uses nested loops.\n","loss, _ = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n","\n","# As a rough sanity check, our loss should be something close to log(10.0).\n","print('loss: %f' % loss)\n","print('sanity check: %f' % (math.log(10.0)))"]},{"cell_type":"markdown","metadata":{"id":"5QJzUHl5I0HH"},"source":["Next, we use gradient checking to debug the analytic gradient of our naive softmax loss function. If you've implemented the gradient correctly, you should see relative errors less than `1e-5`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lj6YpN3q1hVG"},"outputs":[],"source":["import cs639\n","from linear_classifier import softmax_loss_naive\n","\n","cs639.reset_seed(0)\n","W = 0.0005 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n","\n","X_batch = data_dict['X_val'][:128]\n","y_batch = data_dict['y_val'][:128]\n","\n","# YOUR_TURN: Complete the implementation of softmax_loss_naive and implement\n","# a (naive) version of the gradient that uses nested loops.\n","_, grad = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n","\n","f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg=0.0)[0]\n","cs639.grad.grad_check_sparse(f, W, grad, 10)"]},{"cell_type":"markdown","metadata":{"id":"cFcgeajBI-L3"},"source":["Let's perform another gradient check with regularization enabled. Again you should see relative errors less than `1e-5`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik0i21sszZzg"},"outputs":[],"source":["import cs639\n","from linear_classifier import softmax_loss_naive\n","\n","cs639.reset_seed(0)\n","W = 0.0005 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n","reg = 5.0\n","\n","X_batch = data_dict['X_val'][:128]\n","y_batch = data_dict['y_val'][:128]\n","\n","# YOUR_TURN: Complete the gradient compuation part of softmax_loss_naive\n","_, grad = softmax_loss_naive(W, X_batch, y_batch, reg)\n","\n","f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg)[0]\n","cs639.grad.grad_check_sparse(f, W, grad, 10)"]},{"cell_type":"markdown","metadata":{"id":"JQgRzrdRJAm7"},"source":["Then, let's move on to the vectorized form: `softmax_loss_vectorized`. **(20 points)**"]},{"cell_type":"markdown","metadata":{"id":"88xZ0rbLJGKV"},"source":["Now that we have a naive implementation of the softmax loss function and its gradient, implement a vectorized version in softmax_loss_vectorized. The two versions should compute the same results, but the vectorized version should be much faster.\n","\n","The differences between the naive and vectorized losses and gradients should both be less than `1e-6`, and your vectorized implementation should be at least 20x faster than the naive implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGNAe-oP1dds"},"outputs":[],"source":["import cs639\n","from linear_classifier import softmax_loss_naive, softmax_loss_vectorized\n","\n","cs639.reset_seed(0)\n","W = 0.0005 * torch.randn(3073, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n","reg = 0.1\n","\n","X_batch = data_dict['X_val'][:128]\n","y_batch = data_dict['y_val'][:128]\n","\n","# Run and time the naive version\n","torch.cuda.synchronize()\n","tic = time.time()\n","loss_naive, grad_naive = softmax_loss_naive(W, X_batch, y_batch, reg)\n","torch.cuda.synchronize()\n","toc = time.time()\n","ms_naive = 1000.0 * (toc - tic)\n","print('naive loss: %e computed in %fs' % (loss_naive, ms_naive))\n","\n","# Run and time the vectorized version\n","# YOUR_TURN: Complete the implementation of softmax_loss_vectorized\n","torch.cuda.synchronize()\n","tic = time.time()\n","loss_vec, grad_vec = softmax_loss_vectorized(W, X_batch, y_batch, reg)\n","torch.cuda.synchronize()\n","toc = time.time()\n","ms_vec = 1000.0 * (toc - tic)\n","print('vectorized loss: %e computed in %fs' % (loss_vec, ms_vec))\n","\n","# we use the Frobenius norm to compare the two versions of the gradient.\n","loss_diff = (loss_naive - loss_vec).abs().item()\n","grad_diff = torch.norm(grad_naive - grad_vec, p='fro')\n","print('Loss difference: %.2e' % loss_diff)\n","print('Gradient difference: %.2e' % grad_diff)\n","print('Speedup: %.2fX' % (ms_naive / ms_vec))"]},{"cell_type":"markdown","metadata":{"id":"bqZScXKyq6WB"},"source":["Let's check that your implementation of the softmax loss is numerically stable.\n","\n","If either of the following print `nan` then you should double-check the numeric stability of your implementations.\n","\n","We will do this by using a well defined training pipeline.\n","\n","Please complete the implementation of `train_linear_classifier` in `linear_classifer.py`. **(5 points)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCyFPWxxq58R"},"outputs":[],"source":["import cs639\n","from linear_classifier import softmax_loss_naive, softmax_loss_vectorized, train_linear_classifier\n","\n","cs639.reset_seed(0)\n","device = data_dict['X_train'].device\n","dtype = data_dict['X_train'].dtype\n","D = data_dict['X_train'].shape[1]\n","C = 10\n","\n","\n","W_ones = torch.ones(D, C, device=device, dtype=dtype)\n","W, loss_hist = train_linear_classifier(softmax_loss_naive, W_ones,\n","                                       data_dict['X_train'],\n","                                       data_dict['y_train'],\n","                                       learning_rate=1e-7, reg=5.0e4,\n","                                       num_iters=1, verbose=True)\n","\n","\n","W_ones = torch.ones(D, C, device=device, dtype=dtype)\n","W, loss_hist = train_linear_classifier(softmax_loss_vectorized, W_ones,\n","                                       data_dict['X_train'],\n","                                       data_dict['y_train'],\n","                                       learning_rate=1e-7, reg=5.0e4,\n","                                       num_iters=1, verbose=True)\n"]},{"cell_type":"markdown","metadata":{"id":"kR4JGKoek8FB"},"source":["Now lets train a softmax classifier with some default hyperparameters:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kqga1rvjk7b8"},"outputs":[],"source":["import cs639\n","from linear_classifier import softmax_loss_vectorized\n","\n","cs639.reset_seed(0)\n","\n","torch.cuda.synchronize()\n","tic = time.time()\n","\n","\n","W, loss_hist = train_linear_classifier(softmax_loss_vectorized, None,\n","                                       data_dict['X_train'],\n","                                       data_dict['y_train'],\n","                                       learning_rate=1e-12, reg=1.2e4,\n","                                       num_iters=2000, verbose=True)\n","\n","torch.cuda.synchronize()\n","toc = time.time()\n","print('That took %fs' % (toc - tic))"]},{"cell_type":"markdown","metadata":{"id":"QKjxCGwkorCc"},"source":["Plot the loss curve:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K29x-DWNoujL"},"outputs":[],"source":["plt.plot(loss_hist, 'o')\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7WvpBuJWSwfd"},"source":["Let's compute the accuracy of current model. It should be less than 10%. To do that, complete the implementation of `predict_linear_classifier` in `linear_classifier.py`. **(15 points)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zb8kY2MjSvfH"},"outputs":[],"source":["import cs639\n","from linear_classifier import predict_linear_classifier\n","\n","cs639.reset_seed(0)\n","\n","# evaluate the performance on both the training and validation set\n","\n","y_train_pred = predict_linear_classifier(W, data_dict['X_train'])\n","train_acc = 100.0 * (data_dict['y_train'] == y_train_pred).double().mean().item()\n","print('training accuracy: %.2f%%' % train_acc)\n","y_val_pred = predict_linear_classifier(W, data_dict['X_val'])\n","val_acc = 100.0 * (data_dict['y_val'] == y_val_pred).double().mean().item()\n","print('validation accuracy: %.2f%%' % val_acc)"]},{"cell_type":"markdown","metadata":{"id":"IuV0BZvzJirI"},"source":["Now use the validation set to tune hyperparameters (regularization strength and learning rate). You should experiment with different ranges for the learning rates and regularization strengths.\n","\n","Implement `softmax_get_search_params` and `test_one_param_set` in `linear_classifier.py` **(15 + 25 = 40 points)**\n","\n","To get full credit for the assignment, your best model found through cross-validation should achieve an accuracy above 0.36 on the validation set.\n","\n","(Our best model was above 40.1% -- did you beat us?)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68lmNVj31ddu"},"outputs":[],"source":["import os\n","import cs639\n","from linear_classifier import Softmax, softmax_get_search_params, test_one_param_set\n","\n","# YOUR_TURN: find the best learning_rates and regularization_strengths combination\n","#            in 'softmax_get_search_params'\n","learning_rates, regularization_strengths = softmax_get_search_params()\n","num_models = len(learning_rates) * len(regularization_strengths)\n","\n","\n","if num_models > 25:\n","  raise Exception(\"Please do not test/submit more than 25 items at once\")\n","elif num_models < 5:\n","  raise Exception(\"Please present at least 5 parameter sets in your final ipynb\")\n","####\n","\n","\n","i = 0\n","# As before, store your cross-validation results in this dictionary.\n","# The keys should be tuples of (learning_rate, regularization_strength) and\n","# the values should be tuples (train_acc, val_acc)\n","results = {}\n","best_val = -1.0   # The highest validation accuracy that we have seen so far.\n","best_softmax_model = None # The Softmax object that achieved the highest validation rate.\n","num_iters = 2000 # number of iterations\n","\n","for lr in learning_rates:\n","  for reg in regularization_strengths:\n","    i += 1\n","    print('Training Softmax %d / %d with learning_rate=%e and reg=%e'\n","          % (i, num_models, lr, reg))\n","\n","    cs639.reset_seed(0)\n","    cand_softmax_model, cand_train_acc, cand_val_acc = test_one_param_set(Softmax(), data_dict, lr, reg, num_iters)\n","\n","    if cand_val_acc > best_val:\n","      best_val = cand_val_acc\n","      best_softmax_model = cand_softmax_model # save the classifier\n","    results[(lr, reg)] = (cand_train_acc, cand_val_acc)\n","\n","\n","# Print out results.\n","for lr, reg in sorted(results):\n","  train_acc, val_acc = results[(lr, reg)]\n","  print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n","         lr, reg, train_acc, val_acc))\n","\n","print('best validation accuracy achieved during cross-validation: %f' % best_val)\n","\n","# save the best model\n","path = os.path.join(GOOGLE_DRIVE_PATH, 'softmax_best_model.pt')\n","best_softmax_model.save(path)"]},{"cell_type":"markdown","metadata":{"id":"efougAmNCFLo"},"source":["Run the following to visualize your cross-validation results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVhRe3-DBjPr"},"outputs":[],"source":["x_scatter = [math.log10(x[0]) for x in results]\n","y_scatter = [math.log10(x[1]) for x in results]\n","\n","# plot training accuracy\n","marker_size = 100\n","colors = [results[x][0] for x in results]\n","plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n","plt.colorbar()\n","plt.xlabel('log learning rate')\n","plt.ylabel('log regularization strength')\n","plt.title('CIFAR-10 training accuracy')\n","plt.gcf().set_size_inches(8, 5)\n","plt.show()\n","\n","# plot validation accuracy\n","colors = [results[x][1] for x in results] # default size of markers is 20\n","plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n","plt.colorbar()\n","plt.xlabel('log learning rate')\n","plt.ylabel('log regularization strength')\n","plt.title('CIFAR-10 validation accuracy')\n","plt.gcf().set_size_inches(8, 5)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fbOlUcv6J7MM"},"source":["Then, evaluate the performance of your best model on test set. To get full credit for this assignment you should achieve a test-set accuracy above 0.35.\n","\n","(Our best was just around 40.5% -- did you beat us?)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wxkVdB-1ddx"},"outputs":[],"source":["y_test_pred = best_softmax_model.predict(data_dict['X_test'])\n","test_accuracy = torch.mean((data_dict['y_test'] == y_test_pred).double())\n","print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"]},{"cell_type":"markdown","metadata":{"id":"Joo4RbeoKECC"},"source":["Finally, let's visualize the learned weights for each class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDfxI7mR1ddz"},"outputs":[],"source":["w = best_softmax_model.W[:-1,:] # strip out the bias\n","w = w.reshape(3, 32, 32, 10)\n","w = w.transpose(0, 2).transpose(1, 0)\n","\n","w_min, w_max = torch.min(w), torch.max(w)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","  plt.subplot(2, 5, i + 1)\n","\n","  # Rescale the weights to be between 0 and 255\n","  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","  plt.imshow(wimg.type(torch.uint8).cpu())\n","  plt.axis('off')\n","  plt.title(classes[i])"]},{"cell_type":"markdown","source":["Done! Now you can move to part 3 of the problem set, `fully_connected_networks.ipynb`. Remember to save both `linear_classifier.ipynb` and `linear_classifier.py` before exiting."],"metadata":{"id":"fmAijSlJCWX3"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}